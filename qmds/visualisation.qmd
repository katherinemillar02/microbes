---
title: "Visualisation Methods"
about:
  template: marquee
  links:
    - icon: envelope
      text: Email
      href: mailto:Katie.millar@uea.ac.uk
    - icon: github
      text: Github
      href: https://github.com/katherinemillar02
    - text: Bluesky
      href: https://bsky.app/profile/katiemillar.bsky.social
    - icon: linkedin
      text: LinkedIn
      href: https://www.linkedin.com/in/katie-millar-15bb56236/
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(vegan)
```

Methods of visualisation are of uppermost importance, specifically ones that can reduce the dimensionality of data. 



# Principal Component Analysis


¬†

```{r echo=FALSE, out.width="80%", fig.align="center", fig.cap= "Credit: This image was created on Canva."}
knitr::include_graphics("images/PCA2.png")
```

¬†

## A background to PCA

¬†

#### **What is PCA?**

Principal Component Analysis (PCA) is a visualisation method that simplifies complex data by reducing it to just a few key dimensions. It helps you visualise patterns: points that appear close together on the plot are more similar, while those farther apart are more different.  PCA works best when the data exhibits linear relationships, as it captures variation along straight-line directions in the data.

This similarity is often based on **Euclidean distance**, which measures the straight-line distance between data points in the reduced space. PCA ensures that this reduced space still captures the most important variation in the data, so Euclidean distances between points remain meaningful.

¬†

#### **Breaking it down into a few key points...**

-   **PCA** reduces the dimensionality of datasets with a lot of different variables and transforms highly correlated variables together into a smaller set of uncorrelated components.

-   This method clusts together correlated variables by finding the **covariance matrix** of the data, that allows one to see the relationship between different variables, and identifying its principal components, which are defined by their **eigenvalues** and **eigenvectors**.

-   **Eigenvalues** can be used to work out which are the first two, or three (depending on how many are being used) principal components of the dataset, while the **eigenvectors** will be used directly in the calculation of the principal components, along with the raw but **centered data**.

> **Centered data:** where the raw data has had the overall mean of its group substracted from it.

> **Covariance matrix:** a matrix that summarises the relationships between variables in a dataset, looks at how the variables change together.

> **Eigenvalues:** numerical values that represent the amount of data explained by each of the principal components.

> **Eigenvectors:** unit vectors (numerical values) that indicate the direction of the principal components.

¬† To make it easier, lets look at it step by step ...

¬†

#### **The key steps to PCA:**

1.  [**Scale the data**]{style="color:green"} (work out the mean composition of each bacteria).

2.  [**Centre the data**]{style="color:green"} (subtract the column means from the bacteria composition).

3.  Find the [**covariance matrix values**]{style="color:green"} using the [**centered data**]{style="color:green"}.

4.  Calculate the [**eigenvalues**]{style="color:green"} and [**eigenvectors**]{style="color:green"} with the [**covariance matrix values**]{style="color:green"}.

5.  Calculate the [**principal component spaces**]{style="color:green"}.

6.  Generate the [**PCA plot**]{style="color:green"}.

¬†\

```{r echo=FALSE, out.width="80%", fig.align="center", fig.cap= "Credit: This image was created on Canva."}
## My Website Image

knitr::include_graphics("images/PCA-intro.png")


```

¬†

## **PCA in action**

Now we know a little bit more about PCA, let's use PCA to understand some data using the dataset from [Darrington et al. (2022)](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000801#tab2).

¬†

As mentioned throughout this website, in this study, Medflies were reared on six different fruit types, and their microbiota composition was analysed. So this dataset will allow us to use PCA to visualise how microbiota communities differ depending on the fruit substrate if which the Medflies were reared.

If you would like to understand a little more about this study, please see: [**The Home Page**](index.qmd)

¬†

**Table 1:** *Relative abundance (%) of bacterial genera on the surfaces of different fruits. Please scroll sideways to see the entire table.*

::: {style="overflow-x: auto;"}
| Fruit   | *Klebsiella* | *Acinetobacter* | *Pantoea* | *Pseudoxanthomonas* | *Serratia* | *Stenotrophomonas* | *Delftia* | *Burkholderia* | *Sphingomonas* | *Bacillus* | *Sphingobacterium* | *Mycoplasma* |
|------|------|------|------|------|------|------|------|------|------|------|------|------|
| Apricot | 80%          | 10%             | 0%        | 0%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Argan   | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| Grapefruit      | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| Orange  | 70%          | 15%             | 5%        | 5%                  | 5%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Peach   | 60%          | 10%             | 10%       | 10%                 | 5%         | 5%                 | 0%        | 0%             | 5%             | 5%         | 0%                 | 0%           |
| Tang    | 40%          | 10%             | 20%       | 20%                 | 10%        | 5%                 | 0%        | 5%             | 10%            | 0%         | 0%                 | 0%           |
:::

¬†

### Step 1: Scaling the data

As seen above, in the key steps to PCA, our first step is to use our data - and scale it! This involves calculating the **mean composition** of each bacterial relative abundance across all samples.

¬†

**Table 2:** *Relative abundance (%) of bacterial genera on the surfaces of different fruits.  Please scroll sideways to see the entire table.*

::: {#Scale style="overflow-x: auto;"}
| Fruit    | *Klebsiella* | *Acinetobacter* | *Pantoea* | *Pseudoxanthomonas* | *Serratia* | *Stenotrophomonas* | *Delftia* | *Burkholderia* | *Sphingomonas* | *Bacillus* | *Sphingobacterium* | *Mycoplasma* |
|------|------|------|------|------|------|------|------|------|------|------|------|------|
| Apricot  | 80%          | 10%             | 0%        | 0%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Argan    | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| GF       | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| Orange   | 70%          | 15%             | 5%        | 5%                  | 5%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Peach    | 60%          | 10%             | 10%       | 10%                 | 5%         | 5%                 | 0%        | 0%             | 5%             | 5%         | 0%                 | 0%           |
| Tang     | 40%          | 10%             | 20%       | 20%                 | 10%        | 5%                 | 0%        | 5%             | 10%            | 0%         | 0%                 | 0%           |
| **Mean** | **66.7%**    | **10.8%**       | **7.5%**  | **7.5%**            | **3.3%**   | **1.7%**           | **0%**    | **0.8%**       | **2.5%**       | **0.8%**   | **1.7%**           | **3.3%**     |
:::

¬†\
¬†

### Step 2: Centering the data

Our next step is to centre the data. We can do this by subtracting the **mean composition** (which was calculated in Step 1) from each individual value in the data-set. This step adjusts each bacterial taxa so that its average value across all samples becomes zero.

::: {.callout-tip title="Deep Dive into Principal Component Analysis"}
<details>

<summary>Why do we want the average across the samples to be zero?</summary>

PCA is all about variation, not about absolute values. We need to make the average value across zero, so there is no biasing. Say if the abundance of bacteria 1 was 66%, and the bacteria 2 was 5%, if we don't center this - PCA may think that bacteria is way more important, just because there is more of it, but PCA is not about this, it's all about how bacteria varies and works together with one another.

</details>
:::

::: {style="overflow-x: auto;"}
| Fruit   | *Klebsiella* | *Acinetobacter* | *Pantoea* | *Pseudoxanthomonas* | *Serratia* | *Stenotrophomonas* | *Delftia* | *Burkholderia* | *Sphingomonas* | *Bacillus* | *Sphingobacterium* | *Mycoplasma* |
|------|------|------|------|------|------|------|------|------|------|------|------|------|
| Apricot | 13.3%        | -0.8%           | -7.5%     | -7.5%               | -3.3%      | -1.7%              | 0%        | -0.8%          | -2.5%          | -0.8%      | -1.7%              | 1.7%         |
| Argan   | 8.3%         | -0.8%           | -2.5%     | -2.5%               | -3.3%      | -1.7%              | 0%        | -0.8%          | -2.5%          | -0.8%      | 3.3%               | 1.7%         |
| GF      | 8.3%         | -0.8%           | -2.5%     | -2.5%               | -3.3%      | -1.7%              | 0%        | -0.8%          | -2.5%          | -0.8%      | 3.3%               | 1.7%         |
| Orange  | 3.3%         | 4.2%            | -2.5%     | -2.5%               | 1.7%       | -1.7%              | 0%        | -0.8%          | -2.5%          | -0.8%      | -1.7%              | 1.7%         |
| Peach   | -6.7%        | -0.8%           | 2.5%      | 2.5%                | 1.7%       | 3.3%               | 0%        | -0.8%          | 2.5%           | 4.2%       | -1.7%              | -3.3%        |
| Tang    | -26.7%       | -0.8%           | 12.5%     | 12.5%               | 6.7%       | 3.3%               | 0%        | 4.2%           | 7.5%           | -0.8%      | -1.0%              | -3.3%        |
:::

¬†

¬†\
¬†

### Step 3: Calculate the co-variance matrix values

Once the data has been mean-centered, we apply the **covariance formula** to assess the relationships between the bacterial compositions across samples. Covariance measures how much two variables change together.

The population covariance is:

$$
\text{Cov}_{\text{population}} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n}
$$

The sample covariance is:

$$
\text{Cov}_{\text{sample}} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}
$$

¬†

**Breaking down the formula**

$$
( x_i - \bar{x} )
$$

[This represents the centered value of variable ( x ) --- how far one fruit's measurement (e.g., *Klebsiella* abundance) is from the average across all fruits.]{style="color: gray;"}

$$
( y_i - \bar{y} )
$$

[This represents the centered value of variable ( y ) --- for example, how much another bacterial feature (like *Acinetobacter*) deviates from its average.]{style="color: gray;"}

¬†

$$
( n - 1 )
$$ [This is the total number of samples minus one. We divide by ( n - 1 ) instead of ( n ) because we're working with a sample, not the entire population --- this is known as **Bessel's correction**.]{style="color: gray;"}

¬†

Using the co-variance formula, and calculating the mean centered data - we can try calculate our covariance matrix values. We have two options, we can either jump straight to using R, OR, we can first give ourselves a bit of an understanding of how they are calculated manually - so we really understand the method...

¬†

#### **3.1 Calculating the covariance matrix values manually**

With our previously calculated mean centered data, lets work out the co-variance matrix of *Klebsiella* and *Acinetobacter*, and the presence of how these bacteria act together across the different fruits. ¬†

*Covariance Calculation of Klebsiella vs Acinetobacter:*

| Fruit     | *Klebsiella* | *Acinetobacter* |
|-----------|--------------|-----------------|
| Apricot   | 13.3%        | -0.8%           |
| Argan     | 8.3%         | -0.8%           |
| GF        | 8.3%         | -0.8%           |
| Orange    | 3.3%         | 4.2%            |
| Peach     | -6.7%        | -0.8%           |
| Tangerine | -26.7%       | -0.8%           |

Let's quickly tae just these two bacteria from the table above where we calculated the centered values, the top of the formula, tells us to multiple the centered values of one bacteria, by another bacteria. So let's do that for all of them.

Apricot: 
$$
( x_i - \bar{x} ) ( y_i - \bar{y} ) = ( 13.3 ) ( -0.8 ) = -10.64
$$

-   Now lets do these for all of the fruits.

```{=tex}
\begin{align*}
&Apricot:(13.3)*(-0.8) = -10.64 \\
&Argan:(8.3)*(-0.8) = -6.64 \\
&Grapefruit:(8.3)*(-0.8) = -6.64 \\
&Orange:(3.3)*(4.2) = 13.86 \\
&Peach:(-6.7)*(-0.8) = 5.36 \\
&Tangerine:(-26.7)*(-0.8) =  21.36 \\
\end{align*}
```
-   Sum the results

```{=tex}
\begin{align*}
    (-10.64) + (-6.64) + (-6.64) + 13.86 + 5.36 +  21.36  = 16.66 \\
\end{align*}
```
-   We now have the calculation for the top of our formula, now we use the bottom of the formula and divide the result of the sum by (n-1)

$$
\text= \frac{16.66}{6-1} = 3.332
$$

We've worked out the variance of two different species across these 6 different fruits, very good going!

To make sure we have got it down, lets take the variance of the *same* bacteria, and as our formula is bacteria \* bacteria, we can just do squared.

¬†

*Variance: Klebsiella vs Klebsiella* 
\begin{align*}
&Apricot:(13.3)^2 =  176.89 \\
&Argan:(8.3)^2 = 68.89 \\
&Grapefruit:(8.3)^2 = 68.89 \\
&Orange:(3.3)^2 = 10.89 \\
&Peach:(-6.7)^2 = 44.89 \\
&Tangerine:(-26.7)^2 = 712.89 \\
\end{align*}

¬†\begin{align*}
    176.89 + 68.89 + 68.89 + 10.89 + 44.89 + 712.89
    = 1083.34 \\
\end{align*}

$$
 \frac{1083.34}{6-1} = 216.668
$$

We have manually worked out that the covariance matrix of (*Klebsiella*, *Acinetobacter*) = **3.332** and the covariance matrix of (*Klebsiella*, *Klebsiella*) = **216.668**.

Now we know how to work out covariance matrices *manually*, we can be pleased with ourselves, and just use R to work out the rest... This is how to do that!

#### **3.2 Using R to work out the covariance matrix values**

We first put our centered data in a **matrix** for R to read:

```{r}
centered_data <- data.frame(
  Klebsiella = c(13.3, 8.3, 8.3, 3.3, -6.7, -26.7),
  Acinetobacter = c(-0.8, -0.8, -0.8, 4.2, -0.8, -0.8),
  Pantoea = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Pseudoxanthomonas = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Serratia = c(-3.3, -3.3, -3.3, 1.7, 1.7, 6.7),
  Stenotrophomonas = c(-1.7, -1.7, -1.7, -1.7, 3.3, 3.3),
  Delftia = c(0, 0, 0, 0, 0, 0),
  Burkholderia = c(-0.8, -0.8, -0.8, -0.8, -0.8, 4.2),
  Sphingomonas = c(-2.5, -2.5, -2.5, -2.5, 2.5, 7.5),
  Bacillus = c(-0.8, -0.8, -0.8, -0.8, 4.2, -0.8),
  Sphingobacterium = c(-1.7, 3.3, 3.3, -1.7, -1.7, -1.0),
  Mycoplasma = c(1.7, 1.7, 1.7, 1.7, -3.3, -3.3))

centered_data_matrix <- as.matrix(centered_data)


```

<button onclick="toggleVisibility(&#39;centered-data-container&#39;)">
Show/Hide Centered Data Matrix
</button>

::: {#centered-data-container style="display:none;"}
```{r, echo=FALSE}
centered_data <- data.frame(
  Klebsiella = c(13.3, 8.3, 8.3, 3.3, -6.7, -26.7),
  Acinetobacter = c(-0.8, -0.8, -0.8, 4.2, -0.8, -0.8),
  Pantoea = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Pseudoxanthomonas = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Serratia = c(-3.3, -3.3, -3.3, 1.7, 1.7, 6.7),
  Stenotrophomonas = c(-1.7, -1.7, -1.7, -1.7, 3.3, 3.3),
  Delftia = c(0, 0, 0, 0, 0, 0),
  Burkholderia = c(-0.8, -0.8, -0.8, -0.8, -0.8, 4.2),
  Sphingomonas = c(-2.5, -2.5, -2.5, -2.5, 2.5, 7.5),
  Bacillus = c(-0.8, -0.8, -0.8, -0.8, 4.2, -0.8),
  Sphingobacterium = c(-1.7, 3.3, 3.3, -1.7, -1.7, -1.0),
  Mycoplasma = c(1.7, 1.7, 1.7, 1.7, -3.3, -3.3)
)

centered_data_matrix <- as.matrix(centered_data)
centered_data_matrix
```
:::

```{=html}
<script> function toggleVisibility(id) { var el = document.getElementById(id); el.style.display = (el.style.display === 'none') ? 'block' : 'none'; } </script>
```
¬†

Now our centered data is in a **matrix**, of Fruits x Bacteria, we can use this to work out the covariance matrix. I will show you two different ways to work this out in R.

Below shows the code for the actual formula, but written in R.

```{r}
cov_matrix_1 <- (t(centered_data_matrix) %*% centered_data_matrix) / (nrow(centered_data_matrix) - 1)
```

`t()`: this function stands for *transpose*, meaning we will be *flipping* the rows and columns. ie: we have fruits as rows, bacteria as columns, but we want to flip this.

`%*%`: this is the symbol for how we multiply matrices.

Like when we calculated it ourselves above, we our multiplying the centered values of the bacteria from one bacteria to another. Then, dividing the total of this across all the fruits by **n** (the fruits) **-1** (the go-to for calculating a sample size).

Let's look at the output from `cov_matrix_1`:

<button onclick="toggleVisibility(&#39;cov-matrix-output&#39;)">
Show/Hide Manual R `cov_matrix_1` Output
</button>

::: {#cov-matrix-output style="display:none;"}
```{r cov_matrix_1, echo=FALSE}
# Make sure 'centered_data' is defined above
cov_matrix_1 <- (t(centered_data_matrix) %*% centered_data_matrix) / (nrow(centered_data_matrix) - 1)
cov_matrix_1
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```

¬†

Great, now if you compare the covariance matrix values we calculated earlier, for Klebsiella and Acinetobacter, we calculated 3.332 to be the covariance matrix, and for Klebsiella and Klebsiella - we calculated 216.668, this pretty much matches what R has outputted for us!

¬†
Alternatively, R also has the `cov()` function, which will work out the covariance matrix for you.

```{r}
cov_matrix_R <- cov(centered_data_matrix)
```

<button onclick="toggleVisibility(&#39;cov_matrix_R&#39;)">
Show/Hide R Function `cov_matrix_R` Output
</button>

::: {#cov_matrix_R style="display:none;"}
```{r cov_matrix_R, echo=FALSE}
# Make sure 'centered_data' is defined above
cov_matrix_R <- cov(centered_data_matrix)
cov_matrix_R
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```
¬†

And again, as you can see from this output, these numbers are very very similar to the ones we calculated! You now know some different ways to calculate the covariance matrix values.

::: {.callout-tip title="Deep Dive into PCA"}
<details>

<summary>What do the co-variance matrix values show?</summary>

The co-variance matrix here shows how these bacterial species vary together across different fruits. Positive values mean they move in the same direction, negative values mean when one increases, the other tends to decrease. For example lets look at our outputs and focus on *Klebsiella*. *Klebsiella* and *Acinetobacter* move together (3.33), while *Klebsiella* and *Pantonea* move in opposite directions (-100).

</details>
:::

¬†\
¬†

### Step 4: Generating eigenvalues and eigenvectors.

Once we have the covariance matrices, we can proceed with the next step of PCA calculations by calculating the **eigenvalues** and **eigenvectors**! As discussed earlier;

-   The eigen**values** represent the amount of **variance** in each principal component

-   The eigen**vectors** represent the **direction** of principal components.

```{r echo=FALSE, out.width="100%", fig.align="center", fig.cap="This image was generated from ChatGPT."}
   knitr::include_graphics("images/eigen.png") 
```

However, the maths to calculate the eigenvalues and eigenvectors is VERY VERY complicated, so for now, we are just going to use R...

If the maths is something you are keen to know to know, [this page by Ken Kuttler](https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/07%3A_Spectral_Theory/7.01%3A_Eigenvalues_and_Eigenvectors_of_a_Matrix) might help.

Despite this, it's probably still sensible to understand a little more how eigenvalues and eigenvectors work. This leads us to something called spectral theory: a branch of mathematics that studies the spectrum of a matrix or operator, including its eigenvalues and eigenvectors. Spectral theory helps us understand how a matrix behaves by analysing these components, offering deep insights into the structure and transformations represented by the matrix.

We can calculate these using the `eigen()` function in R, which will output both the values and the vectors.

```{r}
eigen <- eigen(cov_matrix_R)
```

<button onclick="toggleVisibility(&#39;eigen&#39;)">
Show/Hide Eigen Output
</button>

::: {#eigen style="display:none;"}
```{r eigen, echo=FALSE}
eigen <- eigen(cov_matrix_R)
eigen
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```
¬†

This will output both eigenvalues and eigenvectors. Alternatively, we can look at the value and vectors individually, for the purpose of calculating our principal components for this analysis lets do this, and go through what each eigen is showing.

**The eigenvalues:**

```{r}
eigen_values <- eigen$values
```

<button onclick="toggleVisibility(&#39;eigen_values&#39;)">
Show/Hide Eigen Values Output
</button>

::: {#eigen_values style="display:none;"}
```{r eigen_values, echo=FALSE}
eigen_values <- eigen$values
eigen_values
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```
¬†

R has given us 12 eigenvalues because our dataset includes 12 different bacteria types, meaning the covariance matrix is 12√ó12. Each eigenvalue corresponds to a principal component and tells us how much variance in the data that component explains. As we can see, the first few eigenvalues are large, capturing most of the variation, while the later ones are extremely small --- close to zero --- and can be ignored. This means we can reduce the dimensionality of our data by focusing on just the components with the largest eigenvalues - we will go more into this later.

¬†

**The eigenvectors:**

```{r}
eigen_vectors <- eigen$vectors
```

<button onclick="toggleVisibility(&#39;eigen_vectors&#39;)">
Show/Hide Eigen Vectors Output
</button>

::: {#eigen_vectors style="display:none;"}
```{r eigen_vectors, echo=FALSE}
eigen_vectors <- eigen$vectors
eigen_vectors
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```

¬†

The eigenvectors are set out the same sort of way as our co-variance matrix, in the order of *Klebsiella*, *Acinetobacter* and *Pantonea*. For example, the first eigenvalue shows **0.78063519** for *Klebsiella* v *Klebsiella*, the lack of negative value means it is in the opposite direction to *Klebsiella* and *Pantonea*, which has a negative value (-0.09110918).

¬†

### Step 5: Calculate the Principal Components

Now we have found the eigenvalues and eigenvectors we work out the principal components! Let's make sure to go through this step by step. 

¬†

*5.1 We first need to take the centered data matrix we made earlier*

`centered_data_matrix`

¬†

*5.2 We then need to work out the **eigenvectors** from the **covariance matrix** (we made the eigenvectors from our centered data):*

```{r}
eigen <- eigen(cov_matrix_R)
eigen_vectors <- eigen$vectors
```

¬†

*5.3 We choose what eigenvalues we are using*

Only the first **4** principal components are actually positive values, principal components 5-12 are negative values, so we can in theory ignore these, and take the first four for now.

```{r}
eigen_values <- eigen$values[1:4]
eigen_values
```

¬†

*5.4 We calculate the actual PC components*

We know that the eigenvectors represent the *direction* of the principal components, so we first calculate our principal components, based on our centered data, and our eigenvectors, by using the matrix multiplication.

```{r}

new_data <- centered_data_matrix %*% eigen_vectors


Fruit <- c("Apricot", "Argan", "Grapefruit", "Orange", "Peach", "Tangerine")

PCA_values <- data.frame(
  Fruit = Fruit,
  PC1 = new_data[, 1],
  PC2 = new_data[, 2])

 
PCA_values
```

¬†

*5.5 Generating the PCA plot.*

But before we do this, earlier I mentioned that we will go back to the eigen values - this is for now. We took four as they were all positive values, but choosing components is usually based on what explains most of the variance, so we will be fine taking the first two. As they make up for the vast majority of the variance.

```{r}
eigen_values <- eigen_values[1:2]

PCA_variance <- round(100 * eigen_values / sum(eigen_values), 2)


```

Using these **2** PCA values, we can get away with generating just the one PCA plot; this is PCA 1 vs. PCA 2.

```{r}
library(ggplot2)

pca_plot <- ggplot(PCA_values, aes(x = PC1, y = PC2, color = Fruit)) +
  geom_point(size = 4) +
  geom_text(aes(label = Fruit), vjust = -1, hjust = 1.2, size = 3, show.legend = FALSE) +
  labs(
    title = "PCA of Fruit Samples",
    x = paste0("Principal Component 1 (", PCA_variance[1], "%)"),
    y = paste0("Principal Component 2 (", PCA_variance[2], "%)")
  ) +
  theme_minimal() +
  theme(
    plot.margin = margin(20, 20, 20, 20),
    plot.title = element_text(hjust = 0.5)
  ) +
  scale_color_brewer(palette = "Dark2")
```
¬†

Now let's see this beautiful plot all together! 
¬†

```{r fig.width=8, fig.height=6, echo=FALSE}

library(ggplot2)
# ONE
# we need the centered data, and we need it as a matriz
centered_data <- data.frame(
  Klebsiella = c(13.3, 8.3, 8.3, 3.3, -6.7, -26.7),
  Acinetobacter = c(-0.8, -0.8, -0.8, 4.2, -0.8, -0.8),
  Pantoea = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Pseudoxanthomonas = c(-7.5, -2.5, -2.5, -2.5, 2.5, 12.5),
  Serratia = c(-3.3, -3.3, -3.3, 1.7, 1.7, 6.7),
  Stenotrophomonas = c(-1.7, -1.7, -1.7, -1.7, 3.3, 3.3),
  Delftia = c(0, 0, 0, 0, 0, 0),
  Burkholderia = c(-0.8, -0.8, -0.8, -0.8, -0.8, 4.2),
  Sphingomonas = c(-2.5, -2.5, -2.5, -2.5, 2.5, 7.5),
  Bacillus = c(-0.8, -0.8, -0.8, -0.8, 4.2, -0.8),
  Sphingobacterium = c(-1.7, 3.3, 3.3, -1.7, -1.7, -1.0),
  Mycoplasma = c(1.7, 1.7, 1.7, 1.7, -3.3, -3.3))

centered_data_matrix <- as.matrix(centered_data)

# TWO
# we also need the eigenvectors, from the covariance matrix 
cov_matrix_R <- cov(centered_data_matrix)

eigen <- eigen(cov_matrix_R)


# THREE
# we select the top two  values, as we have a matrix of 3
eigen_vectors <- eigen$vectors[, 1:2]


# FOUR
# we then get the reduced data, the principal components
reduced_data <- centered_data_matrix %*% eigen_vectors
 # these two are the principal components, so lets make it clearer in the dataframe 



# FIVE
# lets put it all together into a PCA DF

sample_names <- c("Apricot", "Argan", "Grapefruit", "Orange", "Peach", "Tangerine")

PCA_values <- data.frame(
  Sample = sample_names,
  PC1 = reduced_data[, 1],
  PC2 = reduced_data[, 2]
)


pca_plot <- ggplot(PCA_values, aes(x = PC1, y = PC2, color = Fruit)) +
  geom_point(size = 4) +
  geom_text(aes(label = Fruit), vjust = -1, hjust = 1.2, size = 3, show.legend = FALSE) +
  labs(
    title = "PCA of Fruit Samples",
    x = paste0("Principal Component 1 (", PCA_variance[1], "%)"),
    y = paste0("Principal Component 2 (", PCA_variance[2], "%)")
  ) +
  theme_minimal() +
  theme(
    plot.margin = margin(20, 20, 20, 20),
    plot.title = element_text(hjust = 0.5)
  ) +
  scale_color_brewer(palette = "Dark2")

pca_plot


```

¬†
**What can we see?**
¬†

This PCA plot shows variation among different fruit samples based on their principal components. Samples like Orange and Peach cluster closely together, indicating similar microbiome compositions, while Apricot and Argan are positioned farther apart, meaning Medfly reared on these fruits may have greater differences. Grapefruit stands out as more distinct from the other samples. Overall, the plot shows clear groupings and separation among the Medfly microbiotas based on the fruits they were reared on. You can check this against the data shown at the beginning of this page! 

¬†\
¬†

To summarise this chapter, we have taken a deep dive into PCA, and hopefully what actually it is.

¬†\
¬†

## PCA - Test Yourself!

That probably felt like a lot, and well done for getting through!

------------------------------------------------------------------------

**What are the six key steps of PCA you were introduced to?**

::: {#quiz1}
<form id="quiz-form-1">

<input type="radio" name="q1" value="a"> A. 1. Center the data, 2. Scale the data, 3. Calculate the covariance matrix values, 4. Calculate the eigenvalues and eigenvectors, 5. Calculate the principal component spaces, 6. Generate the PCA plot with these <br> <input type="radio" name="q1" value="b"> B. 1. Normalize the data, 2. Perform clustering, 3. Compute distances, 4. Generate a heatmap, 5. Sort by similarity, 6. Interpret clusters visually<br> <input type="radio" name="q1" value="c"> C. 1. One-hot encode variables, 2. Apply k-means clustering, 3. Compute centroids, 4. Evaluate silhouette scores, 5. Merge similar groups, 6. Visualize group centroids<br> <input type="radio" name="q1" value="d"> D. 1. Calculate correlation matrix, 2. Remove outliers, 3. Build decision trees, 4. Rank variable importance, 5. Apply bootstrapping, 6. Use top features in model<br><br>

<button type="button" onclick="checkAnswer1()">

Submit
</button>
</form>

::: {#result1 style="margin-top: 1em; font-weight: bold;"}
:::
:::

```{=html}
<script>
function checkAnswer1() {
  const answer = document.querySelector('#quiz-form-1 input[name="q1"]:checked');
  const result = document.getElementById("result1");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "a") {
    result.textContent = "‚úÖ Correct! Those are the core steps involved in PCA.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>
```

------------------------------------------------------------------------

**Why do we center the data?**

::: {#quiz2}
<form id="quiz-form-2">

<input type="radio" name="q2" value="a"> A. To ensure eigenvalues are always positive<br> <input type="radio" name="q2" value="b"> B. To convert the data into binary form for easier computation<br> <input type="radio" name="q2" value="c"> C. To ensure that each feature has a mean of zero, allowing PCA to correctly find directions of maximum variance<br> <input type="radio" name="q2" value="d"> D. To standardize the range of each variable to between 0 and 1<br><br>

<button type="button" onclick="checkAnswer2()">
Submit
</button>

</form>

::: {#result2 style="margin-top: 1em; font-weight: bold;"}
:::
:::

```{=html}
<script>
function checkAnswer2() {
  const answer = document.querySelector('#quiz-form-2 input[name="q2"]:checked');
  const result = document.getElementById("result2");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "c") {
    result.textContent = "‚úÖ Correct! Centering sets the mean of each feature to zero so PCA can identify directions of maximum variance accurately.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>
```

------------------------------------------------------------------------

**In PCA, what do the eigenvalues and eigenvectors of the covariance matrix represent?**

::: {#quiz3}
<form id="quiz-form-3">

<input type="radio" name="q3" value="a"> A. Eigenvalues determine the correlation between original variables; eigenvectors show how to cluster them<br> <input type="radio" name="q3" value="b"> B. Eigenvalues represent the amount of variance captured by each principal component; eigenvectors define the direction of these components<br> <input type="radio" name="q3" value="c"> C. Eigenvalues are used to normalize the data; eigenvectors eliminate redundancy<br> <input type="radio" name="q3" value="d"> D. Eigenvalues and eigenvectors sort features in descending order of importance based on their original units<br><br>

<button type="button" onclick="checkAnswer3()">
Submit
</button>

</form>

::: {#result3 style="margin-top: 1em; font-weight: bold;"}
:::
:::

```{=html}
<script>
function checkAnswer3() {
  const answer = document.querySelector('#quiz-form-3 input[name="q3"]:checked');
  const result = document.getElementById("result3");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "b") {
    result.textContent = "‚úÖ Correct! Eigenvalues indicate how much variance each component explains, and eigenvectors show their directions.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>
```

------------------------------------------------------------------------

# Non-metric MultiDimensional Scaling

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(vegan)
library(vegan)
library(ggplot2)

```

```{r echo=FALSE, out.width="100%", fig.align="center", fig.cap="Credit: This image was created on Canva.."}

knitr::include_graphics("images/NMDS5.png")

```

¬†

## A background to NMDS

#### **What is NMDS?**

NMDS (Non-metric Multidimensional Scaling) is an **ordination** technique, which is used to visualise **similarities** or **dissimilarities** in data. NMDS is used to reduce the dimensionality of complex data sets, (i.e the abundance of microbes across different sites), to allow for visualisation in a 2D space, (although this can be 3D). Unlike **PCA**, **NMDS** is a non-linear method. It doesn't preserve actual distances between samples, but instead maintains the **rank order of those distances**, which is especially useful when working with complex samples, such as in ecological or microbial community data. 




> **Ordination**: Ordination is a method used to arrange or "order" samples based on patterns of similarity or dissimilarity in multivariate data. It helps reduce complex, high-dimensional data into a few interpretable axes, often for visualsation. ¬†

> **Similarities or dissimilarities**: These refer to how alike (similar) or different (dissimilar) samples are based on measured variables. In ecological or microbial data, this could mean how similar two sites are in terms of species composition. These are often quantified using a distance matrix (e.g., Bray-Curtis, Jaccard, like discussed in a previous chapter). ¬†

> **Rank order of distances**: This refers to the relative ordering of distances between samples, not the actual values. For example, if site A is more similar to site B than to site C, NMDS tries to preserve that order.

::: {.callout-tip title="Deep Dive into Non-metric Multidimensional Scaling"}
<details>

<summary>What does it mean if something is rank-based?</summary>

In a rank-based method like NMDS, the actual numerical distances between samples are not used directly. Instead, the method focuses on the **order** (or rank) of dissimilarities, i.e., which samples are more similar or more different relative to others.

This means that the specific values in the distance matrix are converted to a ranked list, and the ordination tries to preserve that ordering in low-dimensional space. For example, if Sample A is more similar to B than to C, NMDS will try to place A closer to B than to C, regardless of the *actual distance values*.

This approach makes NMDS more flexible and robust to differences in scaling, especially when dealing with ecological or microbiome data that are often noisy.

</details>
:::

#### **The key steps to NMDS:**

1.  **Calculate a dissimilarity matrix**\
    Use a dissimilarity measure (e.g., *Bray-Curtis*) to quantify differences between all sample pairs based on multivariate data (e.g., species abundance).

2.  **Rank the dissimilarity values**\
    Convert the dissimilarity matrix into *ranked distances*, from most similar to least similar.

3.  **Initialise with random coordinates**\
    I an a new 2D space, put samples at **random** starting positions in a low-dimensional space.

4.  **Calculate Euclidean distances**\
    Measure the *Euclidean distances* between all pairs of these random points in the reduced space.

5.  **Rank the Euclidean distances**\
    Rank these distances like done with the original dissimilarity differences in **Step 2**.

6.  **Compare ranked distances using Kruskal's stress**\
    Use Kruskal's stress function to compare the ranked dissimilarities with the ranked Euclidean distances. This metric quantifies how well the configuration preserves the order of the original dissimilarities.

7.  **Iteratively minimise stress**\
    Adjust the configuration repeatedly to *minimise stress*. If the stress is too high, this will be repeated new random configurations.

8.  **Obtain the NMDS solution**\
    The configuration with the *lowest stress* provides the final *NMDS scores* --- a best-fit representation of the rank-based dissimilarities in reduced space.

9.  **Visualise the results**\
    Plot the NMDS scores in 2D (or 3D) space, where the *distance between points* reflects the *relative dissimilarity* between samples.

## **NMDS in action**

To create an NMDS plot for data visualisation, we first need to perform a dissimilarity analysis. In [**Beta Diversity Analysis**](beta.qmd), we calculated the Bray Curtis dissimilarity, so let's start by replicating that to see how Bray Curtis can be calculated directly in R.

In the Beta Diversity chapter, we calculated dissimilarity using a smaller data set of two samples:

::: columns
::: {.column width="50%"}
**Sample 1 - Medfly from Morocco (Argan üå∞)**

**Bacterial composition (relative proportions):**

| Bacterium          | Proportion |
|--------------------|------------|
| *Klebsiella*       | 0.75       |
| *Pantonea*         | 0.15       |
| *Commensalibacter* | 0.10       |
:::

::: {.column width="50%"}
**Sample 2 - Medfly from Greece (Peach üçë)**

**Bacterial composition (relative proportions):**

| Bacterium          | Proportion |
|--------------------|------------|
| *Klebsiella*       | 0.25       |
| *Pantonea*         | 0.50       |
| *Serratia*         | 0.05       |
| *Spinghobacterium* | 0.15       |
:::
:::

For the purpose of demostrating how we can calculate Bray values in R - we will first put all these values into an R dataframe, and use the Bray Curtis method `"bray"` from the `vegdist()` function, from the `vegan()` package.

::: {style="border: 2px solid red; padding: 10px; background-color: #ffe6e6; color: #a10000; border-radius: 5px; margin: 10px 0;"}
<strong>‚ö†Ô∏è Please Note:</strong> The `vegist()` function with Bray-Curtis dissimilarity (`"bray"`) requires no row names in the data frame. To avoid errors, we'll keep track of sample order manually and omit row names in the dataframe.
:::

```{r, message=FALSE}

# Putting together the data frame without the fruit names.
bacteria_df <- data.frame(
  Klebsiella = c(0.75, 0.25),
  Pantonea = c(0.15, 0.50),
  Commensalibacter = c(0.10, 0.00),
  Serratia = c(0.00, 0.05),
  Spinghobacterium = c(0.00, 0.15))

# Using `vegdist()` with our df, and bray curtis for analysis.
bray_curtis <- vegdist(bacteria_df, method = "bray")

# Putting our result back into a matrix, re-adding the fruit names to the rows and colums. 
bray_curtis_matrix <- as.matrix(bray_curtis)
fruit_names <- c("Argan", "Peach")
rownames(bray_curtis_matrix) <- fruit_names
colnames(bray_curtis_matrix) <- fruit_names

bray_curtis_matrix
```

If you can remember, in [**Beta Diversity Analysis**](beta.qmd): we calculated the Bray-Curtis Dissimilarity for these bacteria across these two samples, to be 0.6, our R output shows the value to be 0.589 - which is somewhat similar. So we are on track, and we at least know how to calculate dissimilarity!

Now, since one of the key purposes of NMDS is to reduce complex multivariate data into a simple 2D representation, let's scale things up by bringing in more samples from the [Darrington et al. (2022)](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000801#tab2) dataset.

::: {style="overflow-x: auto;"}
| Fruit   | *Klebsiella* | *Acinetobacter* | *Pantoea* | *Pseudoxanthomonas* | *Serratia* | *Stenotrophomonas* | *Delftia* | *Burkholderia* | *Sphingomonas* | *Bacillus* | *Sphingobacterium* | *Mycoplasma* |
|------|------|------|------|------|------|------|------|------|------|------|------|------|
| Apricot | 80%          | 10%             | 0%        | 0%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Argan   | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| GF      | 75%          | 10%             | 5%        | 5%                  | 0%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 5%                 | 5%           |
| Orange  | 70%          | 15%             | 5%        | 5%                  | 5%         | 0%                 | 0%        | 0%             | 0%             | 0%         | 0%                 | 5%           |
| Peach   | 60%          | 10%             | 10%       | 10%                 | 5%         | 5%                 | 0%        | 0%             | 5%             | 5%         | 0%                 | 0%           |
| Tang    | 40%          | 10%             | 20%       | 20%                 | 10%        | 5%                 | 0%        | 5%             | 10%            | 0%         | 0%                 | 0%           |
:::

¬†

### Step 1: Find the Bray-Curtis dissimilarity values

As demonstrated briefly above, if we are using R to calculate the Bray-Curtis dissimilarity values, we first want to put our data into a dataframe containing the relative abundance of our bacteria for each of the six fruit samples. As we did earlier we will use `"bray"` in the `vegdist()` function in R to calculate the Bray-Curtis dissimilarity values. Ensuring we are making a dataframe where row names have been ommited.

```{r, echo=FALSE, message=FALSE}

df_2 <- data.frame(
  Klebsiella = c(80, 75, 75, 70, 60, 40),
  Acinetobacter = c(10, 10, 10, 15, 10, 10),
  Pantoea = c(0, 5, 5, 5, 10, 20),
  Pseudoxanthomonas = c(0, 5, 5, 5, 10, 20),
  Serratia = c(0, 0, 0, 5, 5, 10),
  Stenotrophomonas = c(0, 0, 0, 0, 5, 5),
  Delftia = c(0, 0, 0, 0, 0, 0),
  Burkholderia = c(0, 0, 0, 0, 0, 5),
  Sphingomonas = c(0, 0, 0, 0, 5, 10),
  Bacillus = c(0, 0, 0, 0, 5, 0),
  Sphingobacterium = c(0, 5, 5, 0, 0, 0),
  Mycoplasma = c(5, 5, 5, 5, 0, 0)
)

bray_curtis <- vegdist(df_2, method = "bray")
bray_curtis_matrix <- as.matrix(bray_curtis)
fruit_names <- c("Apricot", "Argan", "GF", "Orange", "Peach", "Tang")
rownames(bray_curtis_matrix) <- fruit_names
colnames(bray_curtis_matrix) <- fruit_names

bray_curtis_matrix

```

As there are many possible pairwise comparisons between fruit samples, we will have multiple Bray-Curtis values. Although the resulting matrix is symmetric and contains repeated values, it effectively shows how bacterial communities vary across different fruits. The Bray-Curtis values of 0 which are shown diagonaly, comparing each fruit to itself confirms that those samples are identical in their bacterial composition - which would make sense, seeing as it's the same fruit!

::: {.callout-tip title="Deep Dive into Non-metric Multidimensional Scaling"}
<details>

<summary>Did you notice the other 0 Bray-Curtis value?</summary>

If you are fairly mindful, you might notice that we have a Bray Curtis value of 0, across Argan and Grapefruit, this is because - if you go back to the original dataset, the composition of bacteria is the same across these two fruits. Again confirming that Bray Curtis in `veg_dist()` is showing us what we were expecting!

</details>
:::

###  Step 2: Use the dissimilarity values to calculate NMDS.

We have now done pairwise comparisons of the bacterial composition comparing all the different fruit samples. Time to use these to calculate some NMDS stuff!

The `metaMDS()` function:

-   The `metaMDS()` function performs NMDS by using multiple random starts to find a stable solution.

-   It standardises the output for easier interpretation and adds species scores to the site ordination. While `metaMDS()` itself doesn't directly compute NMDS - when we add the relevant information inside this function, it will.

-   We have set `k = 2` because we want to reduce the data to two dimensions (simplifying the data).

-   The `trymax` parameter, set to 100 increases the number of "random starts" to help the algorithm find a stable, low-stress solution (stress is something we will go into more detail on in a bit).

-   Finally, `trace` as `FALSE`, disables the messages where each random start is shown again, because we don't need to see that 100 times.

```{r, message=FALSE, warning=FALSE}
NMDS <- metaMDS(bray_curtis, k = 2, trymax = 100, trace = FALSE)
```

```{r echo=FALSE, out.width="100%", fig.align="center", fig.cap="Credit: This image was created on Canva."}

knitr::include_graphics("images/NMDSbreakdown.png")

```

<button onclick="toggleVisibility(&#39;nmds_output&#39;)">
Show/Hide `NMDS` Output
</button>

::: {#nmds_output style="display: none;"}
```{r nmds, echo=FALSE}
# Calculate NMDS (assuming bray_curtis is already defined)
NMDS <- metaMDS(bray_curtis, k = 2, trymax = 100, trace = FALSE)
NMDS
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  el.style.display = el.style.display === 'none' ? 'block' : 'none';
}
</script>
```
¬†

When we run NMDS, the above is the output we get. Let's go through it a bit!

> **Dimensions 2**- we chose to reduce it to two dimensions (k = 2)

> **Stress 0 -** this is a bit weird, and might indicate our dataframe is a bit too low. We will go into this a bit more later, but low stress is generally meant to be a good thing.

> **Stress type 1, weak ties** this refers to Kruskal's stress formula 1 and "weak ties" means few or no tied ranks

> **Best solution was not repeated after 100 tries** Out of 100 random starts, the best solution occurred in the very first try and was not found again, this could indicate a narrow solution space.

> **The best solution was from try 0** That the best solution came from this suggests metric scaling already gave a perfect (or near-perfect) fit, which is rare.

> **Scaling: centring, PC rotation, halfchange scaling** Scaling involves 3 steps to improve the interpretibility of the NMDS plot; centering shifts the data so that the origin is the center of the configuration, PC rotation rotates the ordination axes based on principal components for easier interpretation and halfchange scaling scales distances so that one unit corresponds to a 50% change in species composition, aiding ecological interpretation.

> **Species:** scores missing: this basically shows because we are measuring differences between sites, not species...

**Getting the NMDS values:** ¬†

We use the NMDS output from the `metaMDS()` function to extract NMDS scores with the `scores()` function, focusing specifically on **sites**, as our aim is to compare species composition among Medfly populations reared on different host fruits. Since we specified `k = 2`, the resulting ordination is visualised in a two-dimensional space.

```{r, message=FALSE, warning=FALSE}
NMDS_scores <- (scores(NMDS, display = "sites"))
```

<button onclick="toggleVisibility(&#39;nmds-container&#39;)">
Show/Hide `NMDS_scores`
</button>

::: {#nmds-container style="display: none;"}
```{r NMDS_scores, echo=FALSE}
nmds_scores <- scores(NMDS, display = "sites")
nmds_scores
```
:::

```{=html}
<script>
function toggleVisibility(id) {
  var el = document.getElementById(id);
  if (el.style.display === 'none') {
    el.style.display = 'block';
  } else {
    el.style.display = 'none';
  }
}
</script>
```
¬†

Great, we now have scores for **NMDS1** and **NMDS2**, which represent the position of each site in the reduced 2D ordination space. However, before jumping into visualising these results, it's worth taking a moment to understand what these axes actually represent.

Unlike principal components in PCA, NMDS axes **do not have a direct linear interpretation**. Instead, **NMDS1** and **NMDS2** are abstract dimensions that best preserve the rank-order of dissimilarities among sites. In our context, this means that sites (i.e., Medfly populations reared on different fruits) that are plotted closer together in this 2D space have more similar compositions between different sites, while those farther apart are more dissimilar.

As we just let R do it for us, we might not really understand how the Bray-Curtis dissimilarity values were used to calculate the NMDS scores - so let's talk it through.

### Step 3: Understanding how dissimilarity values become our NMDS scores.

**How do Bray-Curtis dissimilarity values become NMDS scores?**

While using R makes this process straightforward, it's easy to lose sight of the underlying steps when running functions like `metaMDS()`.

To better understand how Bray-Curtis dissimilarities become NMDS scores, let's walk through the process in detail.

```{r echo=FALSE, out.width="100%", fig.align="center", fig.cap= "Credit: This image was made on Canva."}
knitr::include_graphics("images/summary.png")
```

**What is `metaMDS()` actually doing?**

**Step 3.1**: Get the dissimilarity values.

¬† When `metaMDS()` is running, it starts with the dissimilarity matrix that has been set (in our case Bray-Curtis), and converts the pairwise dissimilarities into ranks. This is because NMDS works on the order of dissimilarities, it tries to preserve which pairs are more similar or dissimilar, rather than the exact numeric values themselves.

Let's take a step back, with some **random numbers** to exemplify this. Below is an example Bray-Curtis dissimilarity matrix for 4 sites. Note that diagonal entries are 0 because a site is identical to itself:

```{r echo=FALSE, fig.cap= "Random Pairwise Bray Curtis Dissimilarity values for 4 Sites."}
bray_matrix <- tibble::tibble(
  Site = c("S1", "S2", "S3", "S4"),
  S1 = c(0.0, 0.3, 0.7, 0.5),
  S2 = c(0.3, 0.0, 0.4, 0.6),
  S3 = c(0.7, 0.4, 0.0, 0.8),
  S4 = c(0.5, 0.6, 0.8, 0.0))
knitr::kable(bray_matrix)
```

¬†

**Step 3.2**: Rank the dissimilarity values. ¬†

Next, `metaMDS()` ranks these pairwise values from most similar (lowest dissimilarity) to most dissimilar (highest dissimilarity). For example, the pair S1 and S2 has the lowest dissimilarity value of 0.3, so is classed as rank 1. While S3 and S4 have the highestr dissimilarity of 0.8, so is classed as rank 6. If you remember from the Beta Diversity chapter, in Bray Curtis 0 is most similar, while 1 is most dissimilar.

```{r echo=FALSE, fig.cap= "A Ranked table of Random Pairwise Bray Curtis Dissimilarity values for 4 Sites."}
bray_ranks <- tibble::tibble(
  Pair = c("S1‚ÄìS2", "S2‚ÄìS3", "S1‚ÄìS4", "S2‚ÄìS4", "S1‚ÄìS3", "S3‚ÄìS4"),
  `Bray-Curtis` = c(0.3, 0.4, 0.5, 0.6, 0.7, 0.8),
  Rank = 1:6)
knitr::kable(bray_ranks)
```

¬†

**Step 3.3**: Put new random co-ordinates in a 2D space. ¬†

In this step, each site (or sample) is randomly assigned coordinates within a 2D space. These starting positions serve as the initial guess for the NMDS algorithm, which will then iteratively adjust them to best reflect the rank-order of dissimilarities among sites.

```{r echo=FALSE, out.width="100%", fig.align="center", fig.cap= "An example of how `metaMDS()` will generate random points. Credit: This image was generated in ChatGPT."}
knitr::include_graphics("images/random.png")
```

¬†

**Step 3.4**: Calculate the **Euclidean distances** between the two points. ¬†

After doing, this it will calculate the **Euclidean distances** between every pair of sites of these random ones. **Euclidean distances** just tells us "how far apart" two points are in our 2D map, and NMDS eventually wants that "how far apart" to reflect how different the original sites were based on Bray-Curtis.

For example, from the random starting configuration example shown below, you have 2D coordinates for each site. For example let's take Site 1, and Site 2.

-   Site 1 is at (-0.25, 0.8)

-   Site 2 is at (0.48, 0.15).

$$
\begin{aligned}
d &= \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\ 
  &= \sqrt{(0.48 - (-0.25))^2 + (0.15 - 0.8)^2} \\
  &= \sqrt{(0.73)^2 + (-0.65)^2} \\
  &= \sqrt{0.5329 + 0.4225} \\
  &= \sqrt{0.9554} \\
  &\approx 0.977
\end{aligned}
$$ *Our Euclidean distance between Site 1 and Site 2 is 0.977*. ¬†

**Step 3.4**: We can then rank the **Euclidean distances** we found

```{r echo=FALSE, fig.cap= "A Ranked table of Eucledian distances for 4 Sites."}
euclidean_ranks <- tibble::tibble(
  Pair = c("S1‚ÄìS2", "S2‚ÄìS3", "S1‚ÄìS4", "S2‚ÄìS4", "S1‚ÄìS3", "S3‚ÄìS4"),
  `Euclidean-Distance` = c(0.977, 0.978, 0.98, 0.986, 0.99, 1),
  Rank = 1:6
)
knitr::kable(euclidean_ranks)
```

¬†

¬† **Step 3.5**: Compare the Bray Curtis and the **Euclidean distances** ranks together.

```{r echo=FALSE, message=FALSE,  fig.cap= "A Ranked comparison table of Bray Curtis dissimilarity values and Eucledian distances for 4 Sites."}
comparison <- tibble::tibble(
  Pair = c("S1‚ÄìS2", "S2‚ÄìS3", "S1‚ÄìS4", "S2‚ÄìS4", "S1‚ÄìS3", "S3‚ÄìS4"),
  `Bray-Curtis` = c(0.3, 0.4, 0.5, 0.6, 0.7, 0.8),
  Dissimilarity_Rank = rank(c(0.3, 0.4, 0.5, 0.6, 0.7, 0.8)),
  Euclidean_Distance = c(0.977, 0.978, 0.98, 0.986, 0.99, 1),
  Euclidean_Rank = rank(c(0.977, 0.978, 0.98, 0.986, 0.99, 1)))
knitr::kable(comparison)
```

¬†

*To help: we can visualise this comparison in a Shephard diagram*

```{r echo=FALSE, message=FALSE}

comparison_plot <- ggplot(comparison, aes(x = `Bray-Curtis`, y = Euclidean_Distance)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Shepard Diagram",
       x = "Ranked Original Bray-Curtis Dissimilarity Values",
       y = "Ranked Euclidean Distances") +
  theme_minimal()

comparison_plot
```

¬†

**Step 3.6**: Using these ranked distances together, calculate the stress value.

¬† With this comparison, we can then calculate a "stress" value, this measures how well the distances in the ordination match the ranked Bray-Curtis dissimilarities. One of the goal of `metaMDS()` is to minimise this stress.

We can calculate this stress value, with something called **Kruskall's Stress Formula**:

$$
\text{Stress} = \sqrt{
  \frac{
    \sum_{i<j} \left( d_{ij} - \hat{d}_{ij} \right)^2
  }{
    \sum_{i<j} d_{ij}^2
  }
}
$$

$$
\text{The original dissimilarity} = d_{ij}
$$

$$
\text{The Euclidean distance between points} = \hat{d}_{ij}
$$ ¬†

To make it a bit easier, lets first put a table together so we can start plugging in the values.

```{r  echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

comparison2 <- comparison %>% 
  mutate(
    Squared_Diff = (`Bray-Curtis` - Euclidean_Distance)^2,
    Bray_Curtis_Sq = `Bray-Curtis`^2
  )

knitr::kable(comparison2)

```

In the table, the Bray-Curtis dissimilarity, Euclidean distance, and squared difference represent the components needed for calculating the numerator, for example, `(0.3 - 0.977)^2 = 0.458`. The denominator is based on the squared value of the Eucledian distances, in this case, `0.977^2 =  0.954529`. However, the calculation will be the **sum** of all of these values. 

```{r  message=FALSE, warning=FALSE}

numerator <- sum(comparison2$Squared_Diff)
denominator <- sum(comparison2$Bray_Curtis_Sq)
stress <- sqrt(numerator / denominator)



0.8069762

```

If the stress value is too high, you go back, and start some of these processes again, that is:

-   Generate new random points in a 2D space.
-   Measure the Eucledian distances.
-   Rank the Eucledian distances.
-   Compare the ranked Eucledian distances with the ranked Bray Curtis dissimilarities.
-   Put these into the stress formula!


The final co-ordinates you end up with are your NMDS scores.

::: {.callout-tip title="Deep Dive into Nonmetric Multidimensional Scaling"}
<details>

<summary>An inquisitive thought might be, why do we use ranks, if these aren't used in the stress formula?</summary>

In NMDS, ranks guide the ordination even though they don't appear directly in the stress formula. Rather than preserving exact dissimilarity values, NMDS preserves the rank order of dissimilarities, for example, which pairs are more or less similar.

NMDS then calculates the Euclidean distances between random points and compares these distances to the original dissimilarities. The key is that the rank order is maintained, not the exact distances.

Stress measures how well the fitted distances match the ordination distances. So, while all distances affect stress, it's the ranking that shapes the configuration, ensuring the order of similarities is preserved as closely as possible.

</details>
:::

¬†

Let's look at a **"real-life" analogy** to help make sense of this process, imagine you're a game of *"Pin the Medfly on the fruit."*. In this game, each **fruit** represents a **Bray-Curtis dissimilarity value**, and the **Medflies** represent the **Euclidean distances**. Your goal is to match the rankings of the Bray-Curtis dissimilarities with the rankings of the Euclidean distances as closely as possible.
```{r echo=FALSE, out.width="60%", fig.align="center", fig.cap= "Credit: This image was made on Canva."}
knitr::include_graphics("images/example1.png")
```
On your first attempt, the match isn't great - your randomly placed Medflies are all over the place! The stress value is high, meaning the ranked Euclidean distances don't align well with the ranked Bray-Curtis values. In other words, you've done a poor job of pinning the Medflies on the fruit.

```{r echo=FALSE, out.width="60%", fig.align="center", fig.cap= "Credit: This image was made on Canva."}
knitr::include_graphics("images/example2.png")
```
So, you try again. You generate new random points, recalculate the Eucledian distances, rank them, and compare these ranks to the Bray-Curtis dissimilarity ranks, you try generate the stress value again - ie, you try to pin the Medfly on the fruit again! This process continues until you find a configuration where the ranks match well and your stress value is low. Thus, finally, the Medflies are now closely aligned with their fruits.

¬†

### Step 4: Visualising your NMDS results!

With our NMDS scores calculated and the dimensionality of the ordination space chosen, we‚Äôre now ready to visualise the results in a plot.


**A summary: what do we need for our plot?**
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Let's get our bacteria dataframe again
bacteria_df <- data.frame(
  Klebsiella = c(0.75, 0.25, 0.5, 0.7, 0.3, 0.75),
  Pantonea = c(0.15, 0.50, 0.05, 0.03, 0.02, 0.05),
  Commensalibacter = c(0.10, 0.02, 0.01, 0.01, 0.01, 0.01),
  Serratia = c(0.01, 0.05, 0.01, 0.01, 0.02, 0.01),
  Spinghobacterium = c(0.01, 0.15, 0.01, 0.02, 0.01, 0.01),
  Acinetobacter = c(0.02, 0.01, 0.01, 0.01, 0.5, 0.01)
)

# Use metaMDS to generate the ordination with lowest stress
invisible(capture.output(
  nmds <- metaMDS(bacteria_df, distance = "bray", k = 2, trymax = 100)
))

# Extract site scores
nmds_scores <- as.data.frame(scores(nmds, display = "sites"))

# Add fruit names
nmds_scores$Fruit <- c("Argan", "Peach", "Apricot", "Grapefruit", "Orange", "Tangerine")

# Plot the NMDS results
nmds_plot <- ggplot(nmds_scores,
                    aes(x = NMDS1, y = NMDS2, colour = Fruit)) +
  geom_point(size = 4) +
  theme_minimal() +
  labs(title = "NMDS Plot of Bacterial Composition",
       subtitle = "Colored by Fruit Type",
       x = "NMDS1", y = "NMDS2")


```

¬†


```{r, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
library(vegan)
library(ggplot2)

bacteria_df <- data.frame(
  Klebsiella = c(0.75, 0.25, 0.5, 0.7, 0.3, 0.75),
  Pantonea = c(0.15, 0.50, 0.05, 0.03, 0.02, 0.05),
  Commensalibacter = c(0.10, 0.02, 0.01, 0.01, 0.01, 0.01),
  Serratia = c(0.01, 0.05, 0.01, 0.01, 0.02, 0.01),
  Spinghobacterium = c(0.01, 0.15, 0.01, 0.02, 0.01, 0.01),
  Acinetobacter = c(0.02, 0.01, 0.01, 0.01, 0.5, 0.01)
)


nmds <- metaMDS(bacteria_df, distance = "bray", k = 2, trymax = 100)

nmds_scores <- as.data.frame(scores(nmds, display = "sites"))

nmds_scores$Fruit <- c("Argan", "Peach", "Apricot", "Grapefruit", "Orange", "Tangerine")


nmds_plot <- ggplot(nmds_scores, 
                    aes(x = NMDS1, y = NMDS2, color = Fruit, label = Fruit)) +
  geom_point(size = 4) + 
  theme_minimal() + 
  labs(title = "NMDS Plot of Bacterial Composition", 
       subtitle = "Colored by Fruit Type",
       x = "NMDS1", y = "NMDS2")

nmds_plot
```

¬†

And finally, we have our NMDS plot! We can see that Tangerine, Grapefruit and Argan have fairly similar bacterial compositions, while Peach and Apricot are more dissimilar. 


# NMDS - Test Yourself! 

------------------------------------------------------------------------


**After you have generated random points in a 2D space, what is the name of the distances you measure between pairwise points.**

::: {#quiz1}
<form id="quiz-form-1">
<input type="radio" name="q1" 
value="a"> A.  Calcu-Lux algorithm <br> <input type="radio" name="q1" 
value="b"> B. Eucledian distances  <br> <input type="radio" name="q1" 
value="c"> C. Blorptastic metrics  <br> <input type="radio" name="q1" 
value="d"> D. Drosophilic distances
<br><br>
<button type="button" onclick="checkAnswer1()">
Submit
</button>
</form>

::: {#result1 style="margin-top: 1em; font-weight: bold;"}
:::
:::

```{=html}
<script>
function checkAnswer1() {
  const answer = document.querySelector('#quiz-form-1 input[name="q1"]:checked');
  const result = document.getElementById("result1");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "b") {
    result.textContent = "‚úÖ Correct! Those are the core steps involved in PCA.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>
```

------------------------------------------------------------------------


**Which of the following best describes the primary goal of Non-metric Multidimensional Scaling?**  

::: {#quiz2}
<form id="quiz-form-2">
  <input type="radio" name="q2" value="a"> A. To maximise the variance explained by principal components<br>
  <input type="radio" name="q2" value="b"> B. To reduce dimensionality by assuming linear relationships between variables<br>
  <input type="radio" name="q2" value="c"> C. To preserve the rank order of dissimilarities between samples in a lower-dimensional space<br>
  <input type="radio" name="q2" value="d"> D. To cluster data into predefined groups based on centroid distances<br><br>
  <button type="button" onclick="checkAnswer2()">Submit</button>
</form>
:::

<div id="result2" style="margin-top: 1em; font-weight: bold;"></div>

```{=html}
<script>
function checkAnswer2() {
  const answer = document.querySelector('#quiz-form-2 input[name="q2"]:checked');
  const result = document.getElementById("result2");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "c") {
    result.textContent = "‚úÖ Correct!";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>

------------------------------------------------------------------------



**Imagine you take some Melon Flies that have been feeding on different fruits: Cucumber, Papaya, Melon, and Squash. You use 16S sequencing to analyse their microbiotas and visualise the results using NMDS. On the plot, flies from Cucumber and Papaya cluster closely together, while Melon and Squash are far apart from each other and from the other groups. What does this suggest about their microbiota compositions?**

<div id="quiz3">
  <form id="quiz-form-3">
    <input type="radio" name="q3" value="a"> A. The microbiotas of all flies are identical regardless of fruit source<br>
    <input type="radio" name="q3" value="b"> B. Cucumber and Papaya-fed flies have more similar microbiota compositions compared to the others<br>
    <input type="radio" name="q3" value="c"> C. Melon and Squash-fed flies have microbiotas that are more abundant in bacteria<br>
    <input type="radio" name="q3" value="d"> D. NMDS is not useful for comparing microbiotas<br><br>
    <button type="button" onclick="checkAnswer3()">Submit</button>
  </form>
</div>

<div id="result3" style="margin-top: 1em; font-weight: bold;"></div>

<script>
function checkAnswer3() {
  const answer = document.querySelector('#quiz-form-3 input[name="q3"]:checked');
  const result = document.getElementById("result3");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "b") {
    result.textContent = "‚úÖ Correct! Closer points on the NMDS plot suggest more similar microbiota compositions.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>

------------------------------------------------------------------------


**In NMDS, why do we compare the ranks of the dissimilarity matrix to the ranks of the distances in the reduced-dimensional space (e.g., Euclidean distances)?**

<div id="quiz4">
  <form id="quiz-form-4">
    <input type="radio" name="q4" value="a"> A. To identify which points are outliers<br>
    <input type="radio" name="q4" value="b"> B. Because NMDS only preserves absolute distance magnitudes<br>
    <input type="radio" name="q4" value="c"> C. To ensure the ranks of pairwise differences are preserved during dimensionality reduction<br>
    <input type="radio" name="q4" value="d"> D. To perform PCA more accurately<br><br>
    <button type="button" onclick="checkAnswer4()">Submit</button>
  </form>
</div>

<div id="result4" style="margin-top: 1em; font-weight: bold;"></div>

<script>
function checkAnswer4() {
  const answer = document.querySelector('#quiz-form-4 input[name="q4"]:checked');
  const result = document.getElementById("result4");

  if (!answer) {
    result.textContent = "‚ö†Ô∏è Please select an answer.";
    result.style.color = "orange";
    return;
  }

  if (answer.value === "c") {
    result.textContent = "‚úÖ Correct! NMDS preserves the rank order of dissimilarities, not the actual distances.";
    result.style.color = "green";
  } else {
    result.textContent = "‚ùå Not quite. Try again.";
    result.style.color = "red";
  }
}
</script>


# References:  
[Eigenvalues and Eigenvectors Maths Book ](https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/07%3A_Spectral_Theory/7.01%3A_Eigenvalues_and_Eigenvectors_of_a_Matrix)

